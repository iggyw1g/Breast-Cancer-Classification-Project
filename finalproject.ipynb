{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa1dab9",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7238507",
   "metadata": {},
   "source": [
    "Riju Pant, Andrew ZiYu Wang, Alisa Sumwalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f211656",
   "metadata": {},
   "source": [
    "## Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d8cba",
   "metadata": {},
   "source": [
    "### Imports and Getting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# from https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download\n",
    "# originally from https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf812180",
   "metadata": {},
   "source": [
    "The dataset we will be using for this project is to do with breast cancer. Breast cancer is a leading cause of death amongst women worldwide, so being able to understand and determine whether one has breast cancer or not is crucial to saving their life. In this notebook, we aim to clasify a tumour as benign or malignant - no harm, or spreads across the body. We got the dataset from Kaggle and it shows 32 columns worth of information for 569 observations. Using this dataset, we want to build classification models to predict the diagnosis, or whether the breast cancer is at a stage that will harm the body or not -we aim to find whether it is malignant or benign. This will be useful for the medical context as predicting the severity of the cancer can help a doctor and patient understand the situation and work towards a solution. Having a working model/models that can give good diagnosis will be key as the key problem with detection of breast cancer is how to classify tumors into the malignant and benign category. Going further, we will understand the dataset and clean it if required, build classification models, and finetune where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200e93d",
   "metadata": {},
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c28843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information about the Dataset:\")\n",
    "print()\n",
    "\n",
    "df.info()\n",
    "\n",
    "row, column = df.shape\n",
    "print()\n",
    "\n",
    "print(f\"Rows = {row}, Columns = {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b3125",
   "metadata": {},
   "source": [
    "Something we find appreciative of this dataset is that everything is a float. When we worked with other datasets previously, we would have some data that should be represented numerically as strings, which would force us to make a new column that has numerical values. Let's learn more about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6816eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf19982",
   "metadata": {},
   "source": [
    "It seemed that there was an extra column that was probably added accidentally. We decided to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526226d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 32\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa71487",
   "metadata": {},
   "source": [
    "### Checking for Null or NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07246e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b77b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62d7ed",
   "metadata": {},
   "source": [
    "The fact that the dataset had no missing entires shocked us at first and we were extremely grateful for the luck, but then when we read the description of the dataset on Kaggle, it had mentioned \"Missing attribute values: none\". Nonetheless, we need to learn more about the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a314511",
   "metadata": {},
   "source": [
    "There are clearly many features in this dataset, meaning training the model could be a bit challenging, especially if only a select few features are what really helps the model learn. We are curious to see more about the id column and something that was not shown in this table - the diagnosis column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4f6c0",
   "metadata": {},
   "source": [
    "After looking at the df.head and df.tail of the dataset, and the value counts of the id column, we realized that the id column is just a patient's ID number, and a feature we do not need for training our model. Therefore, we decided to drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc45027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a126f",
   "metadata": {},
   "source": [
    "When we saw that there are exactly only two classifications, we recalled binary classification, and thought of how we can name our classes as 0 and 1. We decided to just that, where instead of calling a class malignant, we call it 1, and benign as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5dba7",
   "metadata": {},
   "source": [
    "After making sense of some of the columns and what they meant, we tried to understand what the other columns meant. Thanfully, this was mentioned in the description of the dataset on Kaggle.\n",
    "* id = ID number\n",
    "* diagnosis = Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "For the remaining columns, we have \"Ten real-valued features are computed for each cell nucleus\"\n",
    "There are:\n",
    "* radius (mean of distances from center to points on the perimeter)\n",
    "* texture (standard deviation of gray-scale values)\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness (local variation in radius lengths)\n",
    "* compactness (perimeter^2 / area - 1.0)\n",
    "* concavity (severity of concave portions of the contour)\n",
    "* concave points (number of concave portions of the contour)\n",
    "* symmetry\n",
    "* fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance (when we include id and call it field 1), field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "Upon further research and some guessing, we understood that:\n",
    "* radius means the size of the cell nucleus. Malignant cells tend to have larger radii, so radius could be an important feature for us\n",
    "* texture measures the variation in the pixel intensity of a nucleus. This could be an important feature for us because it can help us differentiate between a healthy cell and a malignant one\n",
    "* perimeter means the total length of the boundary of the cell nucleus. This is an important feature with similar reasons to radius, but would we need both radius and perimeter is a question we should tackle later\n",
    "* area means the area of the cell nucleus. This is also important for reasons similar to radius and perimeter; need to think whether to keep as a feature or remove later\n",
    "* smoothness means how smooth or rough the cell nucleus is. Upon researching, breast cancer cells \"... often has angular, irregular, asymmetrical edges, as opposed to being smooth...\" This would definitely be an important feature for our model (https://www.massgeneralbrigham.org/en/about/newsroom/articles/what-does-a-breast-cancer-lump-feel-like)\n",
    "* compactness means how much the shape of the cell nuclei deviates from a perfect circle. This would be a very useful feature for us, especially since breast cancer cells, as mentioned above, often have roughness, meaning they would not be a perfect circle. *A perfect circle would have a compactness of 0*\n",
    "* concavity means how severe the dents in the nucleus cells are. This is also an important feature as breast cancer cells are not exactly smooth\n",
    "* concave points means how many dents there are in the nucleus cells\n",
    "* symmetry means how symmetric the cell nucleus us. If the symmetry is low, it would indicate irregularity and could be a possibility for a malignant cell. This could be an important feature\n",
    "* fractal dimension is an advanced method of measuring how rough the cell nucleus' perimeter is. This is an important feature that could give us reason to not include the perimeter feature as a higher fractal dimension indicates the irregularity of the boundary, which is often a feature of breast cancer cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7462e88",
   "metadata": {},
   "source": [
    "Let's look at the dataset as a whole once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16661f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26551217",
   "metadata": {},
   "source": [
    "It is hard to make sense of this correlation matrix as we have too many columns. Instead, we decided to make a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(\n",
    "    corrMatrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f25bad",
   "metadata": {},
   "source": [
    "This heatmap helps us visualize our correlations, but we realized this is not a good heatmap because it includes our diagnosis values. Since there are 3 sets of 10 real-valued features for each cell nucleus, we decided we can split this into 3 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96abea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftMean = df.columns[1:11]\n",
    "ftMean = list(ftMean)\n",
    "ftSE = df.columns[11:21]\n",
    "ftSE = list(ftSE)\n",
    "ftWorst = df.columns[21:31]\n",
    "ftWorst = list(ftWorst)\n",
    "# print(ftMean)\n",
    "# print(ftSE)\n",
    "# print(ftWorst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3d2dd",
   "metadata": {},
   "source": [
    "Now, we can make a heatmap for the three sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 9))\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=0.7)\n",
    "\n",
    "corrFtMean = df[ftMean].corr()\n",
    "sns.heatmap(\n",
    "    corrFtMean,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[0], \n",
    ")\n",
    "axes[0].set_title('Heatmap for Feature Mean')\n",
    "\n",
    "corrFtSE = df[ftSE].corr()\n",
    "sns.heatmap(\n",
    "    corrFtSE,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Heatmap for Feature Standard Error')\n",
    "\n",
    "corrFtWorst = df[ftWorst].corr()\n",
    "sns.heatmap(\n",
    "    corrFtWorst,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('Heatmap for Feature Worst')\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0])\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "# plt.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038225cd",
   "metadata": {},
   "source": [
    "From the Heatmaps, we can see that\n",
    "- Radius, Area, and Perimeter are highly correlated, which makes sense because area and perimeter are derived using radius\n",
    "- Compactness, Concavity, and Concave Points are highly correlated, which makes sense because these three are indicating that the shape of the cell is not a perfect circle\n",
    "\n",
    "Since we have 10 features which could take time training with, we can reduce the number of features since some of them are highly correlated with each other. Between Radius, Area, and Perimeter, we can choose one of them. Between Compactness, Concavity, and Concave Points, we can choose one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b9eda",
   "metadata": {},
   "source": [
    "Now we will start working on the models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d132eb3",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14293152",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticResults = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, y):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    logSettings = log\n",
    "    log.fit(x_train,y_train)\n",
    "\n",
    "    scoringMetrics = ['accuracy', 'f1', 'neg_log_loss']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(logSettings, X, y, cv=cv, scoring=scoringMetrics)\n",
    "\n",
    "    predictions = log.predict(x_test)\n",
    "    # print(predictions)\n",
    "    logisticResults.append(\n",
    "        [predictions, classification_report(y_test, predictions), confusion_matrix(y_test, predictions), log.predict_proba(x_test)]\n",
    "    )\n",
    "\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV Log-Loss: {scores['test_neg_log_loss'].mean():.4f}\")\n",
    "    print(f\"Log-Loss Standard Deviation: {scores['test_neg_log_loss'].std():.4f}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7304182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLogisticResults(X, y, index):\n",
    "    print(\"Results from our Cross Validate Function\")\n",
    "\n",
    "    logistic(X, y)\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "    print(\"Results from our .fit Function\")\n",
    "\n",
    "    print(logisticResults[index][0])\n",
    "    print(logisticResults[index][1])\n",
    "    print(\"[[TN, FP]\\n [FN, TP]]\\n\")\n",
    "    print(logisticResults[index][2])\n",
    "    # print(f\"Confidence for Each Sample [Percentage Confidence for Class 0, Percentage Confidence for Class 1]: {logisticResults[0][3]}\")\n",
    "\n",
    "    sns.heatmap(logisticResults[index][2], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Observed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printLogisticResults(X, y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printLogisticResults(X, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310481e",
   "metadata": {},
   "source": [
    "Splitting the data sets into three different data sets since the mean, standard error, and worst of these features were computed for each image. So we can study the correlations between these images and see which set would fit the model the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704bea",
   "metadata": {},
   "source": [
    "Accuracy for standard error was 91% which is less than for the mean set (94%). The log loss value for the standard error set was higher than for the mean set, which shows how significant the difference is between the predicted probabilities vs the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printLogisticResults(X, y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c38ac",
   "metadata": {},
   "source": [
    "Intuitive to remove perimeter or area or radius since they are all mathematically related. As you can see the log-loss went down after removing area and perimeter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d5404",
   "metadata": {},
   "source": [
    "### Conclusion for Logistic Regression\n",
    "For the project purpose we want to ensure that all of the cases that are malignant are marked as malignant. The worse case would be a malignant cell marked as benign. Therefore, the priority from the outputs of the model would be the rate of accuracy for '1' within the model. The two models with the best recall number for '1' is mean and worst subsets, with an accuracy of 93%. This means that of all the true malignant cases, 93% of them are marked as malignant.\n",
    "\n",
    "We also want to focus on the false negative rate within the confusion matrices since those would indicate the amount of cases where the cell was malignant but marked as benign. The subset with the best false negative rate is worst compared the others, since 3 out of 70 were false negative. For mean it was 3 out of 66, and for standard error it was 5 out of 69."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89229e49",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d5808",
   "metadata": {},
   "source": [
    "We will be splitting the dataset into three major catagories: The mean, Standard Error, and worst. This way, we can catagorize the data and make the svms more accurate and efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmResults = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae38fef",
   "metadata": {},
   "source": [
    "Lets make the SVM and we will use cross validation which 5 k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X, y, C, kernel):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "    svm = SVC(kernel=kernel, C=C)\n",
    "    svmSettings = svm\n",
    "    svm.fit(x_train,y_train)\n",
    "\n",
    "    predictions = svm.predict(x_test)\n",
    "    # print(predictions)\n",
    "    svmResults.append(\n",
    "        [predictions, classification_report(y_test, predictions), confusion_matrix(y_test, predictions)]\n",
    "    )\n",
    "\n",
    "    scoringMetrics = ['accuracy', 'f1']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(svmSettings, X, y, cv=cv, scoring=scoringMetrics)\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSVMResults(X, y, index, C, kernel):\n",
    "    print(\"Results from our Cross Validate Function\")\n",
    "\n",
    "    svm(X, y, C=C, kernel=kernel)\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "    print(\"Results from our .fit Function\")\n",
    "\n",
    "    print(svmResults[index][0])\n",
    "    print(svmResults[index][1])\n",
    "    print(\"[[TN, FP]\\n [FN, TP]]\\n\")\n",
    "    print(svmResults[index][2])\n",
    "\n",
    "    sns.heatmap(svmResults[index][2], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Observed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371189a",
   "metadata": {},
   "source": [
    "### Lets use SVM and train the data with a margin C of 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40b4a7",
   "metadata": {},
   "source": [
    "Lets use it to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 0, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d97abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 1, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28902e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 2, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c1766",
   "metadata": {},
   "source": [
    "As we can see, the accuracy score is pretty high for all three catagories, especially the \"worst\" data catagory. The f1 scores are also good which is also a good sign. However, despite having a better accuracy and a better average f1 score for ftWorst data set, it has more false negatives than false positives which is not good in the context of predicting brest cancer because marking a cancerous cell as not cancerous is more punishing than marking a non-cancerous cell as cancerous. If this is the case, I think it would be better to use the ftMean instead of the ftWorst SVM as it has better false positive than false negative even though the overall accuracy is worse. Lets test this with a more flexable margin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 3, C=5, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 4, C=5, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd145596",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 5, C=5, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd44a34",
   "metadata": {},
   "source": [
    "We can see that not that the margin is softer, the overall accuracy of some of these has gone down, but the linear one with c = 5.0 for ftMean has a perfect porportion of false negative and false positive as well as being more accurate than it's c = 1.0 version. So far, this is the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13c654",
   "metadata": {},
   "source": [
    "### Let's try polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6065b8",
   "metadata": {},
   "source": [
    "We will try polynomial kernel instead of the linear one to see if the data can be better separated with not just a straight line, but a curved polynomial one. This could potentially create a better accuracy and seapration. One thing we have to watch out is overfitting as it can fit it really well with higher degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31257db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 6, C=1, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 7, C= 1, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab257d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 8, C=1, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd51a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 9, C=5, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 10, C=5, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 11, C=5, kernel='poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462da9ce",
   "metadata": {},
   "source": [
    "### Let's try radial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12006085",
   "metadata": {},
   "source": [
    "Radial is also non-linear, but it is different to the polynomial kernel as it can map to infinite dimensions and is more versatile. However, since the linear kernel is already getting very good accuracy, most likely the data is already linearly separable so non-linear might be overfitting or unessesary. We will try it with kernel to see if there are any improvement regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07879783",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 12, C=1, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 13, C=1, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fe5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 14, C=1, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 15, C=5, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd554f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 16, C=5, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "\n",
    "printSVMResults(X, y, 17, C=5, kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f2cfe",
   "metadata": {},
   "source": [
    "### Conclusion for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c55223",
   "metadata": {},
   "source": [
    "Overall, the polynomial SVM performed similar to the Radial SVM, however the linear one performed the best and is the least complex. Therefore, even if the non-linear SVMs gave us a small increase in accuracy, it might still be better to use lienar as it is less complex and sufficient. Furthermore, with higher complexity, we have to worry more about overfitting. This is probably due to the fact that the data can already be defined linearly and wouldn't require a higher dimension function. The best performing one is the Linear with c = 1.0 with the ftWorst data set as it is more overall accurate and has a good recall rate for both cancerous and non-cancerous cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da3b1e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b38587",
   "metadata": {},
   "source": [
    "After understanding the dataset, we thought good features for the Random Forest would be: radius, texture, smoothness, compactness, symmetry, and fractal dimension. As mentioned previously, some of the features are highly correlated with each other, meaning we can save training time by keeping only one of the highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597c63f",
   "metadata": {},
   "source": [
    "First, we split the dataset into three parts, each according to their category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorsFtMean = ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "predictorsFtSE = ['radius_se', 'texture_se', 'smoothness_se', 'compactness_se', 'symmetry_se', 'fractal_dimension_se']\n",
    "predictorsFtWorst = ['radius_worst', 'texture_worst', 'smoothness_worst', 'compactness_worst', 'symmetry_worst', 'fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f748a45",
   "metadata": {},
   "source": [
    "Now, we set up the Random Forest that we will use to build our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResults = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6930b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X, y, predictors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    RF = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        max_features=len(predictors),\n",
    "        min_samples_leaf=5\n",
    "    )\n",
    "\n",
    "    RFSettings = RF\n",
    "\n",
    "    RF.fit(X_train, y_train)\n",
    "\n",
    "    # predictionsTraining = RF.predict(X_train)\n",
    "\n",
    "    print(\"Results from our Cross Validate Function\")\n",
    "    \n",
    "    scoringMetrics = ['accuracy', 'f1']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(RFSettings, X, y, cv=cv, scoring=scoringMetrics)\n",
    "    # print(f\"Scores for each fold: {scores}\")\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")\n",
    "\n",
    "    print()\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "    print(\"Results from our .fit Function\")\n",
    "    \n",
    "    predictions = RF.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"[[TN, FP]\\n [FN, TP]]\\n\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Observed')\n",
    "    plt.show()\n",
    "\n",
    "    modelResults.append([\n",
    "        [f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\"],\n",
    "        [f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\"],\n",
    "        [f\"Average CV F1: {scores['test_f1'].mean():.4f}\"],\n",
    "        [f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\"]\n",
    "    ])\n",
    "\n",
    "    return pd.Series(RF.feature_importances_, index=predictors).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84815296",
   "metadata": {},
   "source": [
    "### Working with the Mean set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMean = randomForest(df[predictorsFtMean], df['diagnosis'], predictorsFtMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f58147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9525b02",
   "metadata": {},
   "source": [
    "From the results above, using cross validation, we see that our average accuracy after 5 folds CV is 92.09%, with a standard deviation of 2.4%. We also see that the average F1 score is 89.12%, with a standard deviation of 3.2%. We can see that the model is both highly accurate and reliable. The average scores indicate strong predicitve performance, and the low standard deviations indicate good reproducibility across different subsets of the dataset. A 92.09% average accuracy means that the model accurately identified whether a nucleus cell is malignant or benign 92.09% of the times across the 5 validation folds. The low standard deviation indicates that this was not luck and that the model's performance was consistent and stable. This suggests that the model will work well and similar on new unseen data. An 89.12% F1 score indicates that the model has good precision and recall, and that it minimizes False Positives and False Negatives. Essentially, the high F1 score indicates that it is able to correctly identify true positives well. We can say that the model does well to generalize, and would be able to classify between malignant and benign cells with unseen data. Looking at the important feature, it is clear that the most important feature from ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean'] is radius_mean. An advantage of using Random Forests is because it is able to tell us which features were important for the training and predicting. This is why we wanted to use Random Forests as one of our classification methods. From the 6 features that were used to train this model, radius_mean had an overwhelming importance of 69.4%. This indicates that the radius of the nucles cell tells us a lot about whether it is cancerous or not. Moving further, we want to see how the Random Forest works for the other 2 sets, standard error and worst, however before that, we wanted to see whether the features that we removed would also have high importance. First, we decided to train another Random Forest model without omitting any features from the mean set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMeanAll = randomForest(df[ftMean], df['diagnosis'], ftMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMeanAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8560438",
   "metadata": {},
   "source": [
    "We were surprised by the results. The average accuracy and the average F1 score both went up, and not by a small amout. They went up by 2% and 3%, and the standard deviation more than halved. The model works much better, only making the points we made earlier for the RF above even stronger for this one. We can say that this model works better because the time it took to train and make the predictions were almost as long as the previous model. One point that surprised us was that the concave points_mean was the feature that showed the most importance, with almost 79%. This was absolutely shocking as we had removed this feature since we thought Compactness, Concavity, and Concave Points were all strongly correlated to each other and we would be fine using only one of them. Now, we want to see how the top 6 features from this importance list perform with their own mode, comparing it with the previous model that also used 6 features, and with this model that uses all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82865286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftMeanImportant = list(importantFeaturesFtMeanAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4055a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMeanMost = randomForest(df[ftMeanImportant], df['diagnosis'], ftMeanImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab460f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMeanMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f4302",
   "metadata": {},
   "source": [
    "We have gone in the correct direction, as we have been able to maintain both our accuracy and F1 scores while reducing their standard deviations, after reducing the number of features back down to 6. This means that these 6 features are key to determining whether a cell is malignant or benign. Moving forward for the other two sets, we should start our model with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f58ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorsFtSE = list(importantFeaturesFtMeanMost.index.str.replace(\"_mean\", \"_se\"))\n",
    "predictorsFtWorst = list(importantFeaturesFtMeanMost.index.str.replace(\"_mean\", \"_worst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde2768",
   "metadata": {},
   "source": [
    "### Working with the Standard Error set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSE = randomForest(df[predictorsFtSE], df['diagnosis'], predictorsFtSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e0e42",
   "metadata": {},
   "source": [
    "The results were slightly underwhelming, but nonetheless, the model is not bad. In fact, on its own is pretty good, but when comparing to our previous results, the model shows some underperformance. This might be because we are assuming that the best predictors from our mean features are applicable for the others. We should make a model with all the standard error features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSEAll = randomForest(df[ftSE], df['diagnosis'], ftSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082606ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSEAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008df2a5",
   "metadata": {},
   "source": [
    "The results are disappointing as our accuracy and F1 score have dropped after using all the features for our model. This may be because we are using more features that do not really have much importance to making predictions. The addition of features could be an introduction to noise. These features may have little to no importance for the prediction of malginant or benign, and the model may be trying to find relationships between them. Also, adding features can lead to overcomplicating the model, which is why it is underperforming. A phenomenon our steps have shown agreement with is the Curse of Dimensionality. As we increased our features, the data became more spread out, making it harder for the model to find meaningful patterns. Our next step will be to see how the model behaves with the top 6 important features that we have seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b29c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftSEImportant = list(importantFeaturesFtSEAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSEMost = randomForest(df[ftSEImportant], df['diagnosis'], ftSEImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSEMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f05f2",
   "metadata": {},
   "source": [
    "Overall, after seeing the results of the three models using the standard error features, it is clear that the mean features are much better at making predictions. We could change the hyperparameters of the Random Forest, however doing so would mean that the model becomes more complex and not worth the training power. Let's continue with the worst set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbea03",
   "metadata": {},
   "source": [
    "### Working with the Worst set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ebbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorst = randomForest(df[predictorsFtWorst], df['diagnosis'], predictorsFtWorst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd2529",
   "metadata": {},
   "source": [
    "Considering how the standard error features model with the most important features from the mean set went, we were well surprised by the results, immediately seeing a 95.8% accuracy and 94.4% F1 score. These are values we welcome with open arms, especially since this was the first model with the worst features. We hypothesize that if we use all the features, we would see the Curse of Dimensionality playing an effect in the results of the next model. Nonetheless, we will see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorstAll = randomForest(df[ftWorst], df['diagnosis'], ftWorst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorstAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4716c",
   "metadata": {},
   "source": [
    "We were surprised to see that the Curse of Dimensionality did not play an effect, and rather, the accuracy increase, though ever so slightly. This was impressive because it shows that the worst features are also able to make good models and good predictions. Something that surprised us was that the top 6 important features for the worst set is coincidentally also the same as the top 6 important features for the mean set. This would mean that these 6 are strong predictors for distinguishing between a malignant cell and a benign cell. Overall, though the introduction of 4 additional features does technically improve the accuracy of the model, and the training time is basically the same as mentioned when building models for the mean dataset, soemthing we realized later on is that our sample size is also small. With a larger sample size, the time it would take to train a model with 6 features vs 10 features would only increase apart. The difference in the two models we made with the worst features are hardly justifible for saying the 10 feature model is worth using. We can safely say that the 6 feature model will perform better and is less expensive to train. To be consistent with how we did the mean and standard error models, we will once again build a model with the top 6 important features we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftWorstImportant = list(importantFeaturesFtWorstAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd90435",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorstMost = randomForest(df[ftWorstImportant], df['diagnosis'], ftWorstImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorstMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd175a",
   "metadata": {},
   "source": [
    "Funnily, when we trained our model with the same 6 features we used for the first one, we yielded slightly different results. This time, we yielded the same results as the 10 feature model. This is interesting as it goes to show that the four extra features that the 10 feature model had where of no no signficance to the model, and removing those 4 had no effects on the prediction of the nucles cells. A possible reason why the results yielded are different, even though we set the random state to 42, from the first random forests is possibly because the order of introducing the predictors may be different. The ordering of this changes the order in which the Random Forests does its operations. Nonetheless, the importance of the features both times were very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17593478",
   "metadata": {},
   "source": [
    "### Conclusion from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelResults[2])\n",
    "print(importantFeaturesFtMeanMost)\n",
    "print(\"-\"*50)\n",
    "print(modelResults[8])\n",
    "print(importantFeaturesFtWorstMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdeae03",
   "metadata": {},
   "source": [
    "Looking at the two models that yielded the best results, it seems that for both the mean features and worst features, the same category of features were the most important - concave points, texture, area, perimeter, radius, and concavity. These make sense when we think about it as the number of dents a cell nucleus has is a good indicator of whether it is healthy, and the more dents we have, the more likely it is that it is malignant. The texture, area, perimeter, radius, and concavity also share the same reasons as to why they are important. The shape of the cell nucleus is vital to determining wheter it is cancerous or not, and thanks to Random Forests, we were able to determine that these 6 features hold the most important for classifying new, unseen data. Moving forward, it would be interesting to see if when we use these 6 features, would they also yield better results than using all 10 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a1dbf",
   "metadata": {},
   "source": [
    "### Testing 10 vs 6 Features on Logistic Regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432685c",
   "metadata": {},
   "source": [
    "We will use the cross validate function to see which model on average is better. Since we have the code KFold(n_splits=5, shuffle=True, random_state=42), all calls for the cross_validate function results in the same folds of the data. Meaning, we can with confidence determine whether using all 10 features or the 6 best features for each of the three sets is better or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5d307",
   "metadata": {},
   "source": [
    "First, let's try Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87669de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftMean], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtMeanMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftSE], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtSEMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftWorst], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f46848",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtWorstMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67edb87",
   "metadata": {},
   "source": [
    "From the above results, it seems that there are no significant differences between training the model on 10 features or 6 features. The difference is extremely insignificant to the point where we may want to prioritize using the model that uses less features as it yields the same results as the model with 10. If we had larger datasets to train with, then the model with 6 features would be better, however with our sample size, it may be best to use the model with all 10 features as it still technically yields the better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb33f5a",
   "metadata": {},
   "source": [
    "Now, let's try SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftMean], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtMeanMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cab3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftSE], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c11f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtSEMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382860fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftWorst], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtWorstMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a5337",
   "metadata": {},
   "source": [
    "The same as logistic regression can be concluded for SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5f3a0",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659cd8",
   "metadata": {},
   "source": [
    "Our project aimed to develop three different classification Machine Learning models - Logistic Regression, Support Vector Machines, and Random Forests - for identifying whether a breast cancer tumor is malignant or bengign. The main goal of our project was to make a model that can effectively identify with the data, if a tumor is harmful or not. This is critical to help many people in the medical context, and we wanted to make a model that can work well on unseen data. From our results, we could see that our Support Vector Machine model had the highest accuracy and F1 score, with values 96.3% and 95.1% respectively. While the Random Forest model and Logistic Regression model were close second and third with accuracies of 95.6% and 95.1% respecitvely, and F1 scores of 94.1% and 93.4% respectively, it was not as good as our SVM. All three models were effective when using the worst set and not the mean and standard error sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7313e",
   "metadata": {},
   "source": [
    "The effectiveness of the SVM can be linked to its ability to work in high dimensional spaces and the ability to formulate a hyperplane separating two classes. By using a linear kernel, the SVM was able to find the best straight line to distinguish malignant and benign classes. This means that the dataset can be separated linearly and playing around with the margin of the hyperplane was the most effective strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3db647",
   "metadata": {},
   "source": [
    "We used two methods to see which models were better with which sets (mean, SE, worst). We used cross_validate, and fitting the model normally. The cross_validate function from SciKit Learn. What this function does is it automates the process of cross validation for us, and we can see the average accuracies and F1 scores. This was helpful for us to determine the performance of the models. This is how we were able to boil down our best models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14451ec8",
   "metadata": {},
   "source": [
    "When analyzing our data, we were pleased with our accuracy and F1 scores, however we later realized that our project does not only focus on accuracy, but also precision and recall. In the medical context, we do not want False Negative, where we incorrectly classify a malginant tumor as benign. Therefore, the recall is the most important metric for our models. We focussed on our recall values for class 1, or malignant. This is because we want to minimize our False Negatives. Having a higher recall for class 1 help us achieve this goal. With a Random Forest classifier, we got a recall of 95% when working with the mean set, however this was with only the top 6 best features that we handpicked in the beginning. With a SVM, we got a recall of 93% when working with the worst set. There were many SVM models that showed a recall of 93% for class 1, but this model had a recall for class 0 of 99%, and that the weighted average was slightly higher. Also the precision was higher for the model with a 99% recall for class 0, so we identified that as the best model for SVMs. For logisitc regression, we got a recall of 93% when working with the worst set as well. We identified that in the medical context, the Random Forest model had the best recall for malignant cases with a value of 95%, minimizing the risk of misdiagnosing a patient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2ef86",
   "metadata": {},
   "source": [
    "Some further research that can be done is hyperparameter tuning. We can play around more with the hyperparameters of each model, hoping for a better result. Using techniques like GridSearchCV for the softness of the SVMs margin could be effective. Since we only worked with C=1 and C=5, finding the optimal C could show SVMs to have a leverage over the other two models. Another step forward could be validating our models on new, unseen data from an external dataset. This would be beneficial as it can help finetune our model even more. Another step forward could be to compare and analyze the three models we decided were the best in consideration of recall for class 1 (malginant tumors). The issue at stand is that the random forest model was trained with 6 of the 10 features from the mean set, and the SVM and logistic regression were trained with all 10 features from the worst set. We would need to find a better way to compare our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
