{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa1dab9",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7238507",
   "metadata": {},
   "source": [
    "Riju Pant, Andrew ZiYu Wang, Alisa Sumwalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f211656",
   "metadata": {},
   "source": [
    "## Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d8cba",
   "metadata": {},
   "source": [
    "### Imports and Getting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# from https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download\n",
    "# originally from https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf812180",
   "metadata": {},
   "source": [
    "The dataset we will be using for this project is to do with breast cancer. Breast cancer is a leading cause of death amongst women worldwide, so being able to understand and determine whether one has breast cancer or not is crucial to saving their life. In this notebook, we aim to clasify a tumour as benign or malignant - no harm, or spreads across the body. We got the dataset from Kaggle and it shows 32 columns worth of information for 569 observations. Using this dataset, we want to build classification models to predict the diagnosis, or whether the breast cancer is at a stage that will harm the body or not -we aim to find whether it is malignant or benign. This will be useful for the medical context as predicting the severity of the cancer can help a doctor and patient understand the situation and work towards a solution. Having a working model/models that can give good diagnosis will be key as the key problem with detection of breast cancer is how to classify tumors into the malignant and benign category. Going further, we will understand the dataset and clean it if required, build classification models, and finetune where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200e93d",
   "metadata": {},
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c28843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information about the Dataset:\")\n",
    "print()\n",
    "\n",
    "df.info()\n",
    "\n",
    "row, column = df.shape\n",
    "print()\n",
    "\n",
    "print(f\"Rows = {row}, Columns = {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b3125",
   "metadata": {},
   "source": [
    "Something we find appreciative of this dataset is that everything is a float. When we worked with other datasets previously, we would have some data that should be represented numerically as strings, which would force us to make a new column that has numerical values. Let's learn more about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6816eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf19982",
   "metadata": {},
   "source": [
    "It seemed that there was an extra column that was probably added accidentally. We decided to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526226d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 32\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa71487",
   "metadata": {},
   "source": [
    "### Checking for Null or NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07246e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b77b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62d7ed",
   "metadata": {},
   "source": [
    "The fact that the dataset had no missing entires shocked us at first and we were extremely grateful for the luck, but then when we read the description of the dataset on Kaggle, it had mentioned \"Missing attribute values: none\". Nonetheless, we need to learn more about the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a314511",
   "metadata": {},
   "source": [
    "There are clearly many features in this dataset, meaning training the model could be a bit challenging, especially if only a select few features are what really helps the model learn. We are curious to see more about the id column and something that was not shown in this table - the diagnosis column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4f6c0",
   "metadata": {},
   "source": [
    "After looking at the df.head and df.tail of the dataset, and the value counts of the id column, we realized that the id column is just a patient's ID number, and a feature we do not need for training our model. Therefore, we decided to drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc45027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a126f",
   "metadata": {},
   "source": [
    "When we saw that there are exactly only two classifications, we recalled binary classification, and thought of how we can name our classes as 0 and 1. We decided to just that, where instead of calling a class malignant, we call it 1, and benign as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5dba7",
   "metadata": {},
   "source": [
    "After making sense of some of the columns and what they meant, we tried to understand what the other columns meant. Thanfully, this was mentioned in the description of the dataset on Kaggle.\n",
    "* id = ID number\n",
    "* diagnosis = Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "For the remaining columns, we have \"Ten real-valued features are computed for each cell nucleus\"\n",
    "There are:\n",
    "* radius (mean of distances from center to points on the perimeter)\n",
    "* texture (standard deviation of gray-scale values)\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness (local variation in radius lengths)\n",
    "* compactness (perimeter^2 / area - 1.0)\n",
    "* concavity (severity of concave portions of the contour)\n",
    "* concave points (number of concave portions of the contour)\n",
    "* symmetry\n",
    "* fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance (when we include id and call it field 1), field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "Upon further research and some guessing, we understood that:\n",
    "* radius means the size of the cell nucleus. Malignant cells tend to have larger radii, so radius could be an important feature for us\n",
    "* texture measures the variation in the pixel intensity of a nucleus. This could be an important feature for us because it can help us differentiate between a healthy cell and a malignant one\n",
    "* perimeter means the total length of the boundary of the cell nucleus. This is an important feature with similar reasons to radius, but would we need both radius and perimeter is a question we should tackle later\n",
    "* area means the area of the cell nucleus. This is also important for reasons similar to radius and perimeter; need to think whether to keep as a feature or remove later\n",
    "* smoothness means how smooth or rough the cell nucleus is. Upon researching, breast cancer cells \"... often has angular, irregular, asymmetrical edges, as opposed to being smooth...\" This would definitely be an important feature for our model (https://www.massgeneralbrigham.org/en/about/newsroom/articles/what-does-a-breast-cancer-lump-feel-like)\n",
    "* compactness means how much the shape of the cell nuclei deviates from a perfect circle. This would be a very useful feature for us, especially since breast cancer cells, as mentioned above, often have roughness, meaning they would not be a perfect circle. *A perfect circle would have a compactness of 0*\n",
    "* concavity means how severe the dents in the nucleus cells are. This is also an important feature as breast cancer cells are not exactly smooth\n",
    "* concave points means how many dents there are in the nucleus cells\n",
    "* symmetry means how symmetric the cell nucleus us. If the symmetry is low, it would indicate irregularity and could be a possibility for a malignant cell. This could be an important feature\n",
    "* fractal dimension is an advanced method of measuring how rough the cell nucleus' perimeter is. This is an important feature that could give us reason to not include the perimeter feature as a higher fractal dimension indicates the irregularity of the boundary, which is often a feature of breast cancer cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7462e88",
   "metadata": {},
   "source": [
    "Let's look at the dataset as a whole once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16661f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26551217",
   "metadata": {},
   "source": [
    "It is hard to make sense of this correlation matrix as we have too many columns. Instead, we decided to make a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(\n",
    "    corrMatrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f25bad",
   "metadata": {},
   "source": [
    "This heatmap helps us visualize our correlations, but we realized this is not a good heatmap because it includes our diagnosis values. Since there are 3 sets of 10 real-valued features for each cell nucleus, we decided we can split this into 3 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96abea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftMean = df.columns[1:11]\n",
    "ftMean = list(ftMean)\n",
    "ftSE = df.columns[11:21]\n",
    "ftSE = list(ftSE)\n",
    "ftWorst = df.columns[21:31]\n",
    "ftWorst = list(ftWorst)\n",
    "# print(ftMean)\n",
    "# print(ftSE)\n",
    "# print(ftWorst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3d2dd",
   "metadata": {},
   "source": [
    "Now, we can make a heatmap for the three sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 9))\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=0.7)\n",
    "\n",
    "corrFtMean = df[ftMean].corr()\n",
    "sns.heatmap(\n",
    "    corrFtMean,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[0], \n",
    ")\n",
    "axes[0].set_title('Heatmap for Feature Mean')\n",
    "\n",
    "corrFtSE = df[ftSE].corr()\n",
    "sns.heatmap(\n",
    "    corrFtSE,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Heatmap for Feature Standard Error')\n",
    "\n",
    "corrFtWorst = df[ftWorst].corr()\n",
    "sns.heatmap(\n",
    "    corrFtWorst,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    cbar = True,\n",
    "    annot_kws={'size': 6},\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('Heatmap for Feature Worst')\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0])\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "# plt.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038225cd",
   "metadata": {},
   "source": [
    "From the Heatmaps, we can see that\n",
    "- Radius, Area, and Perimeter are highly correlated, which makes sense because area and perimeter are derived using radius\n",
    "- Compactness, Concavity, and Concave Points are highly correlated, which makes sense because these three are indicating that the shape of the cell is not a perfect circle\n",
    "\n",
    "Since we have 10 features which could take time training with, we can reduce the number of features since some of them are highly correlated with each other. Between Radius, Area, and Perimeter, we can choose one of them. Between Compactness, Concavity, and Concave Points, we can choose one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b9eda",
   "metadata": {},
   "source": [
    "Now we will start working on the models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d132eb3",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e6deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, y):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "    log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    log.fit(x_train,y_train)\n",
    "\n",
    "    scoringMetrics = ['accuracy', 'f1', 'neg_log_loss']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(log, X, y, cv=cv, scoring=scoringMetrics)\n",
    "\n",
    "    predictions = log.predict(x_test)\n",
    "    print(predictions)\n",
    "\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV Log-Loss: {scores['test_neg_log_loss'].mean():.4f}\")\n",
    "    print(f\"Log-Loss Standard Deviation: {scores['test_neg_log_loss'].std():.4f}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftMean]\n",
    "y = df['diagnosis']\n",
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# log.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions1 = log.predict(x_test)\n",
    "# print(predictions1)\n",
    "# proba = log.predict_proba(x_test)\n",
    "# print(\"The log-loss is:\", log_loss(y_test, proba))\n",
    "# accuracy1 = accuracy_score(y_test, predictions1)\n",
    "# print(\"accuracy is:\", accuracy1)\n",
    "logistic(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftSE]\n",
    "y = df['diagnosis']\n",
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# log.fit(x_train,y_train)\n",
    "\n",
    "# predictions2 = log.predict(x_test)\n",
    "# print(predictions2)\n",
    "# proba = log.predict_proba(x_test)\n",
    "# print(\"The log-loss is:\", log_loss(y_test, proba))\n",
    "# accuracy2  = accuracy_score(y_test, predictions2)\n",
    "# print(\"accuracy is:\", accuracy2)\n",
    "logistic(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310481e",
   "metadata": {},
   "source": [
    "Splitting the data sets into three different data sets since the mean, standard error, and worst of these features were computed for each image. So we can study the correlations between these images and see which set would fit the model the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704bea",
   "metadata": {},
   "source": [
    "Accuracy for standard error was 91% which is less than for the mean set (94%). The log loss value for the standard error set was higher than for the mean set, which shows how significant the difference is between the predicted probabilities vs the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[ftWorst]\n",
    "y = df['diagnosis']\n",
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# log.fit(x_train,y_train)\n",
    "\n",
    "# predictions3 = log.predict(x_test)\n",
    "# print(predictions3)\n",
    "# proba = log.predict_proba(x_test)\n",
    "# print(\"The log-loss is:\", log_loss(y_test, proba))\n",
    "# accuracy3 = accuracy_score(y_test, predictions3)\n",
    "# print(\"accuracy is:\", accuracy3)\n",
    "logistic(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c38ac",
   "metadata": {},
   "source": [
    "Intuitive to remove perimeter or area or radius since they are all mathematically related. As you can see the log-loss went down after removing area and perimeter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89229e49",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d5808",
   "metadata": {},
   "source": [
    "We will be splitting the dataset into three major catagories: The mean, Standard Error, and worst. This way, we can catagorize the data and make the svms more accurate and efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ec998",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['diagnosis']\n",
    "\n",
    "# meanX_train,meanX_test,meanY_train,meanY_test = train_test_split(df[ftMean],y,test_size=0.35,random_state=42)\n",
    "# XSE_train,XSE_test,YSE_train,YSE_test = train_test_split(df[ftSE],y,test_size=0.35,random_state=42)\n",
    "# worstX_train,worstX_test,worstY_train,worstY_test = train_test_split(df[ftWorst],y,test_size=0.35,random_state=42)\n",
    "\n",
    "# svm_classifier_mean = SVC(kernel='linear', C=1.0)\n",
    "# svm_classifier_mean.fit(meanX_train, meanY_train)\n",
    "\n",
    "# svm_classifier_SE = SVC(kernel='linear', C=1.0)\n",
    "# svm_classifier_SE.fit(XSE_train, YSE_train)\n",
    "\n",
    "# svm_classifier_worst = SVC(kernel='linear', C=1.0)\n",
    "# svm_classifier_worst.fit(worstX_train, worstY_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X, y, C, kernel):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    svm = SVC(kernel=kernel, C=C)\n",
    "    svm.fit(x_train,y_train)\n",
    "\n",
    "    scoringMetrics = ['accuracy', 'f1']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(svm, X, y, cv=cv, scoring=scoringMetrics)\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371189a",
   "metadata": {},
   "source": [
    "### Lets use SVM and train the data with a margin C of 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40b4a7",
   "metadata": {},
   "source": [
    "Lets use it to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredict = svm_classifier_mean.predict(meanX_test)\n",
    "# print(accuracy_score(meanY_test, yPredict))\n",
    "# print(classification_report(meanY_test, yPredict))\n",
    "svm(df[ftMean], y, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d97abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredict2 = svm_classifier_SE.predict(XSE_test)\n",
    "# print(accuracy_score(YSE_test, yPredict2))\n",
    "# print(classification_report(YSE_test, yPredict))\n",
    "svm(df[ftSE], y, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28902e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredict3 = svm_classifier_worst.predict(worstX_test)\n",
    "# print(accuracy_score(worstY_test, yPredict3))\n",
    "# print(classification_report(worstY_test, yPredict))\n",
    "svm(df[ftWorst], y, C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c1766",
   "metadata": {},
   "source": [
    "As we can see, the accuracy score is pretty high for all three catagories, especially the worst catagory. This is an indicator for potential high variance due to overfitting. Lets test this with a more flexable margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb04838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier_mean2 = SVC(kernel='linear', C=5.0)\n",
    "# svm_classifier_mean2.fit(meanX_train, meanY_train)\n",
    "\n",
    "# svm_classifier_SE2 = SVC(kernel='linear', C=5)\n",
    "# svm_classifier_SE2.fit(XSE_train, YSE_train)\n",
    "\n",
    "# svm_classifier_worst2 = SVC(kernel='linear', C=5)\n",
    "# svm_classifier_worst2.fit(worstX_train, worstY_train)\n",
    "\n",
    "# yPredict4 = svm_classifier_mean2.predict(meanX_test)\n",
    "# print(accuracy_score(meanY_test, yPredict4))\n",
    "# print(classification_report(meanY_test, yPredict4))\n",
    "\n",
    "# yPredict5 = svm_classifier_SE2.predict(XSE_test)\n",
    "# print(accuracy_score(YSE_test, yPredict5))\n",
    "# print(classification_report(YSE_test, yPredict5))\n",
    "\n",
    "# yPredict6 = svm_classifier_worst2.predict(worstX_test)\n",
    "# print(accuracy_score(worstY_test, yPredict6))\n",
    "# print(classification_report(worstY_test, yPredict6))\n",
    "\n",
    "svm(df[ftMean], y, C=5, kernel='linear')\n",
    "print()\n",
    "svm(df[ftSE], y, C=5, kernel='linear')\n",
    "print()\n",
    "svm(df[ftWorst], y, C=5, kernel='linear')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13c654",
   "metadata": {},
   "source": [
    "# The result haven't changed much so we will try a polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b29e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier_mean2 = SVC(kernel='poly', C=5.0)\n",
    "# svm_classifier_mean2.fit(meanX_train, meanY_train)\n",
    "\n",
    "# svm_classifier_SE2 = SVC(kernel='poly', C=5)\n",
    "# svm_classifier_SE2.fit(XSE_train, YSE_train)\n",
    "\n",
    "# svm_classifier_worst2 = SVC(kernel='poly', C=5)\n",
    "# svm_classifier_worst2.fit(worstX_train, worstY_train)\n",
    "\n",
    "# yPredict4 = svm_classifier_mean2.predict(meanX_test)\n",
    "# print(accuracy_score(meanY_test, yPredict4))\n",
    "# print(classification_report(meanY_test, yPredict4))\n",
    "\n",
    "# yPredict5 = svm_classifier_SE2.predict(XSE_test)\n",
    "# print(accuracy_score(YSE_test, yPredict5))\n",
    "# print(classification_report(YSE_test, yPredict5))\n",
    "\n",
    "# yPredict6 = svm_classifier_worst2.predict(worstX_test)\n",
    "# print(accuracy_score(worstY_test, yPredict6))\n",
    "# print(classification_report(worstY_test, yPredict6))\n",
    "\n",
    "\n",
    "svm(df[ftMean], y, C=5, kernel='poly')\n",
    "print()\n",
    "svm(df[ftSE], y, C=5, kernel='poly')\n",
    "print()\n",
    "svm(df[ftWorst], y, C=5, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f38c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier_mean2 = SVC(kernel='rbf', C=5.0)\n",
    "# svm_classifier_mean2.fit(meanX_train, meanY_train)\n",
    "\n",
    "# svm_classifier_SE2 = SVC(kernel='rbf', C=5)\n",
    "# svm_classifier_SE2.fit(XSE_train, YSE_train)\n",
    "\n",
    "# svm_classifier_worst2 = SVC(kernel='rbf', C=5)\n",
    "# svm_classifier_worst2.fit(worstX_train, worstY_train)\n",
    "\n",
    "# yPredict4 = svm_classifier_mean2.predict(meanX_test)\n",
    "# print(accuracy_score(meanY_test, yPredict4))\n",
    "# print(classification_report(meanY_test, yPredict4))\n",
    "\n",
    "# yPredict5 = svm_classifier_SE2.predict(XSE_test)\n",
    "# print(accuracy_score(YSE_test, yPredict5))\n",
    "# print(classification_report(YSE_test, yPredict5))\n",
    "\n",
    "# yPredict6 = svm_classifier_worst2.predict(worstX_test)\n",
    "# print(accuracy_score(worstY_test, yPredict6))\n",
    "# print(classification_report(worstY_test, yPredict6))\n",
    "\n",
    "svm(df[ftMean], y, C=5, kernel='rbf')\n",
    "print()\n",
    "svm(df[ftSE], y, C=5, kernel='rbf')\n",
    "print()\n",
    "svm(df[ftWorst], y, C=5, kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da3b1e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b38587",
   "metadata": {},
   "source": [
    "After understanding the dataset, we thought good features for the Random Forest would be: radius, texture, smoothness, compactness, symmetry, and fractal dimension. As mentioned previously, some of the features are highly correlated with each other, meaning we can save training time by keeping only one of the highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597c63f",
   "metadata": {},
   "source": [
    "First, we split the dataset into three parts, each according to their category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorsFtMean = ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "predictorsFtSE = ['radius_se', 'texture_se', 'smoothness_se', 'compactness_se', 'symmetry_se', 'fractal_dimension_se']\n",
    "predictorsFtWorst = ['radius_worst', 'texture_worst', 'smoothness_worst', 'compactness_worst', 'symmetry_worst', 'fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f748a45",
   "metadata": {},
   "source": [
    "Now, we set up the Random Forest that we will use to build our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResults = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6930b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X, y, predictors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    RF = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        max_features=len(predictors),\n",
    "        min_samples_leaf=5\n",
    "    )\n",
    "\n",
    "    RF.fit(X_train, y_train)\n",
    "\n",
    "    # predictionsTraining = RF.predict(X_train)\n",
    "    # predictions = RF.predict(X_test)\n",
    "    scoringMetrics = ['accuracy', 'f1']\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(RF, X, y, cv=cv, scoring=scoringMetrics)\n",
    "    # print(f\"Scores for each fold: {scores}\")\n",
    "    print(f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\")\n",
    "\n",
    "    print(f\"Average CV F1: {scores['test_f1'].mean():.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\")\n",
    "\n",
    "    modelResults.append([\n",
    "        [f\"Average CV Accuracy: {scores['test_accuracy'].mean():.4f}\"],\n",
    "        [f\"Accuracy Standard Deviation: {scores['test_accuracy'].std():.4f}\"],\n",
    "        [f\"Average CV F1: {scores['test_f1'].mean():.4f}\"],\n",
    "        [f\"F1 Standard Deviation: {scores['test_f1'].std():.4f}\"]\n",
    "    ])\n",
    "\n",
    "    return pd.Series(RF.feature_importances_, index=predictors).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84815296",
   "metadata": {},
   "source": [
    "### Working with the Mean set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMean = randomForest(df[predictorsFtMean], df['diagnosis'], predictorsFtMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f58147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9525b02",
   "metadata": {},
   "source": [
    "From the results above, using cross validation, we see that our average accuracy after 5 folds CV is 92.09%, with a standard deviation of 2.4%. We also see that the average F1 score is 89.12%, with a standard deviation of 3.2%. We can see that the model is both highly accurate and reliable. The average scores indicate strong predicitve performance, and the low standard deviations indicate good reproducibility across different subsets of the dataset. A 92.09% average accuracy means that the model accurately identified whether a nucleus cell is malignant or benign 92.09% of the times across the 5 validation folds. The low standard deviation indicates that this was not luck and that the model's performance was consistent and stable. This suggests that the model will work well and similar on new unseen data. An 89.12% F1 score indicates that the model has good precision and recall, and that it minimizes False Positives and False Negatives. Essentially, the high F1 score indicates that it is able to correctly identify true positives well. We can say that the model does well to generalize, and would be able to classify between malignant and benign cells with unseen data. Looking at the important feature, it is clear that the most important feature from ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean'] is radius_mean. An advantage of using Random Forests is because it is able to tell us which features were important for the training and predicting. This is why we wanted to use Random Forests as one of our classification methods. From the 6 features that were used to train this model, radius_mean had an overwhelming importance of 69.4%. This indicates that the radius of the nucles cell tells us a lot about whether it is cancerous or not. Moving further, we want to see how the Random Forest works for the other 2 sets, standard error and worst, however before that, we wanted to see whether the features that we removed would also have high importance. First, we decided to train another Random Forest model without omitting any features from the mean set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMeanAll = randomForest(df[ftMean], df['diagnosis'], ftMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMeanAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8560438",
   "metadata": {},
   "source": [
    "We were surprised by the results. The average accuracy and the average F1 score both went up, and not by a small amout. They went up by 2% and 3%, and the standard deviation more than halved. The model works much better, only making the points we made earlier for the RF above even stronger for this one. We can say that this model works better because the time it took to train and make the predictions were almost as long as the previous model. One point that surprised us was that the concave points_mean was the feature that showed the most importance, with almost 79%. This was absolutely shocking as we had removed this feature since we thought Compactness, Concavity, and Concave Points were all strongly correlated to each other and we would be fine using only one of them. Now, we want to see how the top 6 features from this importance list perform with their own mode, comparing it with the previous model that also used 6 features, and with this model that uses all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82865286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftMeanImportant = list(importantFeaturesFtMeanAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4055a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtMeanMost = randomForest(df[ftMeanImportant], df['diagnosis'], ftMeanImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab460f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtMeanMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f4302",
   "metadata": {},
   "source": [
    "We have gone in the correct direction, as we have been able to maintain both our accuracy and F1 scores while reducing their standard deviations, after reducing the number of features back down to 6. This means that these 6 features are key to determining whether a cell is malignant or benign. Moving forward for the other two sets, we should start our model with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f58ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorsFtSE = list(importantFeaturesFtMeanMost.index.str.replace(\"_mean\", \"_se\"))\n",
    "predictorsFtWorst = list(importantFeaturesFtMeanMost.index.str.replace(\"_mean\", \"_worst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde2768",
   "metadata": {},
   "source": [
    "### Working with the Standard Error set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSE = randomForest(df[predictorsFtSE], df['diagnosis'], predictorsFtSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e0e42",
   "metadata": {},
   "source": [
    "The results were slightly underwhelming, but nonetheless, the model is not bad. In fact, on its own is pretty good, but when comparing to our previous results, the model shows some underperformance. This might be because we are assuming that the best predictors from our mean features are applicable for the others. We should make a model with all the standard error features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSEAll = randomForest(df[ftSE], df['diagnosis'], ftSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082606ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSEAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008df2a5",
   "metadata": {},
   "source": [
    "The results are disappointing as our accuracy and F1 score have dropped after using all the features for our model. This may be because we are using more features that do not really have much importance to making predictions. The addition of features could be an introduction to noise. These features may have little to no importance for the prediction of malginant or benign, and the model may be trying to find relationships between them. Also, adding features can lead to overcomplicating the model, which is why it is underperforming. A phenomenon our steps have shown agreement with is the Curse of Dimensionality. As we increased our features, the data became more spread out, making it harder for the model to find meaningful patterns. Our next step will be to see how the model behaves with the top 6 important features that we have seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b29c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftSEImportant = list(importantFeaturesFtSEAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtSEMost = randomForest(df[ftSEImportant], df['diagnosis'], ftSEImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtSEMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f05f2",
   "metadata": {},
   "source": [
    "Overall, after seeing the results of the three models using the standard error features, it is clear that the mean features are much better at making predictions. We could change the hyperparameters of the Random Forest, however doing so would mean that the model becomes more complex and not worth the training power. Let's continue with the worst set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbea03",
   "metadata": {},
   "source": [
    "### Working with the Worst set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ebbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorst = randomForest(df[predictorsFtWorst], df['diagnosis'], predictorsFtWorst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd2529",
   "metadata": {},
   "source": [
    "Considering how the standard error features model with the most important features from the mean set went, we were well surprised by the results, immediately seeing a 95.8% accuracy and 94.4% F1 score. These are values we welcome with open arms, especially since this was the first model with the worst features. We hypothesize that if we use all the features, we would see the Curse of Dimensionality playing an effect in the results of the next model. Nonetheless, we will see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorstAll = randomForest(df[ftWorst], df['diagnosis'], ftWorst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorstAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4716c",
   "metadata": {},
   "source": [
    "We were surprised to see that the Curse of Dimensionality did not play an effect, and rather, the accuracy increase, though ever so slightly. This was impressive because it shows that the worst features are also able to make good models and good predictions. Something that surprised us was that the top 6 important features for the worst set is coincidentally also the same as the top 6 important features for the mean set. This would mean that these 6 are strong predictors for distinguishing between a malignant cell and a benign cell. Overall, though the introduction of 4 additional features does technically improve the accuracy of the model, and the training time is basically the same as mentioned when building models for the mean dataset, soemthing we realized later on is that our sample size is also small. With a larger sample size, the time it would take to train a model with 6 features vs 10 features would only increase apart. The difference in the two models we made with the worst features are hardly justifible for saying the 10 feature model is worth using. We can safely say that the 6 feature model will perform better and is less expensive to train. To be consistent with how we did the mean and standard error models, we will once again build a model with the top 6 important features we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftWorstImportant = list(importantFeaturesFtWorstAll.index[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd90435",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesFtWorstMost = randomForest(df[ftWorstImportant], df['diagnosis'], ftWorstImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importantFeaturesFtWorstMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd175a",
   "metadata": {},
   "source": [
    "Funnily, when we trained our model with the same 6 features we used for the first one, we yielded slightly different results. This time, we yielded the same results as the 10 feature model. This is interesting as it goes to show that the four extra features that the 10 feature model had where of no no signficance to the model, and removing those 4 had no effects on the prediction of the nucles cells. A possible reason why the results yielded are different, even though we set the random state to 42, from the first random forests is possibly because the order of introducing the predictors may be different. The ordering of this changes the order in which the Random Forests does its operations. Nonetheless, the importance of the features both times were very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17593478",
   "metadata": {},
   "source": [
    "### Conclusion from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelResults[2])\n",
    "print(importantFeaturesFtMeanMost)\n",
    "print(\"-\"*50)\n",
    "print(modelResults[8])\n",
    "print(importantFeaturesFtWorstMost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdeae03",
   "metadata": {},
   "source": [
    "Looking at the two models that yielded the best results, it seems that for both the mean features and worst features, the same category of features were the most important - concave points, texture, area, perimeter, radius, and concavity. These make sense when we think about it as the number of dents a cell nucleus has is a good indicator of whether it is healthy, and the more dents we have, the more likely it is that it is malignant. The texture, area, perimeter, radius, and concavity also share the same reasons as to why they are important. The shape of the cell nucleus is vital to determining wheter it is cancerous or not, and thanks to Random Forests, we were able to determine that these 6 features hold the most important for classifying new, unseen data. Moving forward, it would be interesting to see if when we use these 6 features, would they also yield better results than using all 10 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a1dbf",
   "metadata": {},
   "source": [
    "### Testing 10 vs 6 Features on Logistic Regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5d307",
   "metadata": {},
   "source": [
    "First, let's try Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87669de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftMean], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtMeanMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftSE], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtSEMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[ftWorst], df['diagnosis']) # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f46848",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(df[importantFeaturesFtWorstMost.index], df['diagnosis']) # 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67edb87",
   "metadata": {},
   "source": [
    "From the above results, it seems that there are no significant differences between training the model on 10 features or 6 features. The difference is extremely insignificant to the point where we may want to prioritize using the model that uses less features as it yields the same results as the model with 10. If we had larger datasets to train with, then the model with 6 features would be better, however with our sample size, it may be best to use the model with all 10 features as it still technically yields the better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb33f5a",
   "metadata": {},
   "source": [
    "Now, let's try SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftMean], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtMeanMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cab3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftSE], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c11f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtSEMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382860fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[ftWorst], df['diagnosis'], C=5, kernel='linear') # 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(df[importantFeaturesFtWorstMost.index], df['diagnosis'], C=5, kernel='linear') # 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a5337",
   "metadata": {},
   "source": [
    "The same as logistic regression can be concluded for SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5f3a0",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659cd8",
   "metadata": {},
   "source": [
    "We used three models to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
